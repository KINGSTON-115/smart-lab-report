# ğŸ§ª AI å¤§æ¨¡å‹é›†æˆæ¨¡å—
# AI LLM Integration Module

"""
æ”¯æŒæ¥å…¥å¤šç§å¤§è¯­è¨€æ¨¡å‹ï¼š
- OpenAI (GPT-3.5/GPT-4)
- Claude (Anthropic)
- æœ¬åœ°æ¨¡å‹ (Ollama)
- é€šä¹‰åƒé—® (é˜¿é‡Œ)
- æ™ºè°± AI (ChatGLM)
- ç™¾åº¦æ–‡å¿ƒä¸€è¨€
"""

import os
import json
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
import pandas as pd

# ç¯å¢ƒå˜é‡è¯»å–
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY", "")
QWEN_API_KEY = os.environ.get("QWEN_API_KEY", "")
ZHIPU_API_KEY = os.environ.get("ZHIPU_API_KEY", "")
BAIDU_API_KEY = os.environ.get("BAIDU_API_KEY", "")


@dataclass
class AIConfig:
    """AI é…ç½®"""
    provider: str = "openai"  # openai, claude, qwen, zhipu, baidu, local
    model: str = "gpt-3.5-turbo"
    api_key: str = ""
    base_url: str = ""
    temperature: float = 0.7
    max_tokens: int = 2000
    timeout: int = 60
    
    def __post_init__(self):
        # è‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è·å– API Key
        if not self.api_key:
            if self.provider == "openai":
                self.api_key = OPENAI_API_KEY
            elif self.provider == "claude":
                self.api_key = ANTHROPIC_API_KEY
            elif self.provider == "qwen":
                self.api_key = QWEN_API_KEY
            elif self.provider == "zhipu":
                self.api_key = ZHIPU_API_KEY
            elif self.provider == "baidu":
                self.api_key = BAIDU_API_KEY


@dataclass
class ExperimentData:
    """å®éªŒæ•°æ®"""
    raw_data: pd.DataFrame
    title: str = ""
    description: str = ""
    subject: str = ""  # physics, chemistry, biology, etc.


@dataclass
class AnalysisResult:
    """åˆ†æç»“æœ"""
    phenomenon: str = ""           # å®éªŒç°è±¡æè¿°
    conclusion: str = ""           # å®éªŒç»“è®º
    trend: str = ""                # æ•°æ®è¶‹åŠ¿
    anomaly: str = ""              # å¼‚å¸¸æ•°æ®
    suggestion: str = ""           # æ”¹è¿›å»ºè®®
    confidence: float = 0.0       # ç½®ä¿¡åº¦
    raw_response: str = ""         # åŸå§‹å“åº”


class BaseLLMProvider(ABC):
    """LLM æä¾›å•†åŸºç±»"""
    
    @abstractmethod
    def chat(self, messages: List[Dict], **kwargs) -> str:
        """å‘é€å¯¹è¯è¯·æ±‚"""
        pass


class OpenAIProvider(BaseLLMProvider):
    """OpenAI æä¾›å•†"""
    
    def __init__(self, config: AIConfig):
        self.config = config
        self._client = None
    
    def _get_client(self):
        if self._client is None:
            try:
                from openai import OpenAI
                self._client = OpenAI(
                    api_key=self.config.api_key,
                    base_url=self.config.base_url or None,
                    timeout=self.config.timeout
                )
            except ImportError:
                raise ImportError("è¯·å®‰è£… OpenAI: pip install openai")
        return self._client
    
    def chat(self, messages: List[Dict], **kwargs) -> str:
        client = self._get_client()
        response = client.chat.completions.create(
            model=self.config.model,
            messages=messages,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens
        )
        return response.choices[0].message.content


class ClaudeProvider(BaseLLMProvider):
    """Anthropic Claude æä¾›å•†"""
    
    def __init__(self, config: AIConfig):
        self.config = config
        self._client = None
    
    def _get_client(self):
        if self._client is None:
            try:
                from anthropic import Anthropic
                self._client = Anthropic(
                    api_key=self.config.api_key,
                    timeout=self.config.timeout
                )
            except ImportError:
                raise ImportError("è¯·å®‰è£… Anthropic: pip install anthropic")
        return self._client
    
    def chat(self, messages: List[Dict], **kwargs) -> str:
        client = self._get_client()
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        content = [m["content"] for m in messages]
        response = client.messages.create(
            model=self.config.model,
            max_tokens=self.config.max_tokens,
            temperature=self.config.temperature,
            messages=content
        )
        return response.content[0].text


class QwenProvider(BaseLLMProvider):
    """é˜¿é‡Œé€šä¹‰åƒé—®æä¾›å•†"""
    
    def __init__(self, config: AIConfig):
        self.config = config
        self._client = None
    
    def _get_client(self):
        if self._client is None:
            try:
                from openai import OpenAI
                self._client = OpenAI(
                    api_key=self.config.api_key,
                    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
                )
            except ImportError:
                raise ImportError("è¯·å®‰è£… OpenAI SDK")
        return self._client
    
    def chat(self, messages: List[Dict], **kwargs) -> str:
        client = self._get_client()
        response = client.chat.completions.create(
            model=self.config.model,
            messages=messages,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens
        )
        return response.choices[0].message.content


class ZhipuProvider(BaseLLMProvider):
    """æ™ºè°± AI æä¾›å•†"""
    
    def __init__(self, config: AIConfig):
        self.config = config
        self._client = None
    
    def _get_client(self):
        if self._client is None:
            try:
                from openai import OpenAI
                self._client = OpenAI(
                    api_key=self.config.api_key,
                    base_url="https://open.zhipu.ai.com/v4"
                )
            except ImportError:
                raise ImportError("è¯·å®‰è£… OpenAI SDK")
        return self._client
    
    def chat(self, messages: List[Dict], **kwargs) -> str:
        client = self._get_client()
        response = client.chat.completions.create(
            model=self.config.model,
            messages=messages,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens
        )
        return response.choices[0].message.content


class LocalProvider(BaseLLMProvider):
    """æœ¬åœ°æ¨¡å‹æä¾›å•† (Ollama)"""
    
    def __init__(self, config: AIConfig):
        self.config = config
    
    def chat(self, messages: List[Dict], **kwargs) -> str:
        try:
            from openai import OpenAI
            client = OpenAI(
                api_key="ollama",
                base_url=self.config.base_url or "http://localhost:11434/v1"
            )
            response = client.chat.completions.create(
                model=self.config.model,
                messages=messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens
            )
            return response.choices[0].message.content
        except Exception as e:
            raise ConnectionError(f"æ— æ³•è¿æ¥åˆ°æœ¬åœ°æ¨¡å‹: {e}")


class AILabAnalyzer:
    """AI å®éªŒåˆ†æå™¨ - ä¸»ç±»"""
    
    PROVIDERS = {
        "openai": OpenAIProvider,
        "claude": ClaudeProvider,
        "qwen": QwenProvider,
        "zhipu": ZhipuProvider,
        "local": LocalProvider,
    }
    
    DEFAULT_MODELS = {
        "openai": "gpt-3.5-turbo",
        "claude": "claude-3-sonnet-20240229",
        "qwen": "qwen-turbo",
        "zhipu": "glm-4",
        "local": "llama2",
    }
    
    def __init__(self, config: AIConfig = None):
        self.config = config or AIConfig()
        self.provider = self._create_provider()
        self._available = self._check_availability()
    
    def _create_provider(self) -> BaseLLMProvider:
        """åˆ›å»º LLM æä¾›å•†"""
        provider_class = self.PROVIDERS.get(self.config.provider, OpenAIProvider)
        return provider_class(self.config)
    
    def _check_availability(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ç”¨"""
        if not self.config.api_key and self.config.provider != "local":
            return False
        try:
            # ç®€å•æµ‹è¯•
            self._test_connection()
            return True
        except Exception:
            return False
    
    def _test_connection(self):
        """æµ‹è¯•è¿æ¥"""
        messages = [{"role": "user", "content": "Hello"}]
        return self.provider.chat(messages)
    
    def _format_data_for_ai(self, data: pd.DataFrame, title: str = "") -> str:
        """æ ¼å¼åŒ–æ•°æ®ç»™ AI"""
        if data is None or data.empty:
            return "æ— æ•°æ®"
        
        # åŸºæœ¬ç»Ÿè®¡
        stats = []
        numeric_cols = data.select_dtypes(include=['number']).columns
        for col in numeric_cols:
            col_data = data[col]
            stats.append(f"- {col}: å‡å€¼={col_data.mean():.4f}, æ ‡å‡†å·®={col_data.std():.4f}, èŒƒå›´=[{col_data.min()}, {col_data.max()}]")
        
        # æ•°æ®é¢„è§ˆ
        preview = f"æ•°æ®å½¢çŠ¶: {data.shape[0]} è¡Œ Ã— {data.shape[1]} åˆ—\n"
        preview += "åˆ—å: " + ", ".join(data.columns.tolist()) + "\n"
        preview += "ç»Ÿè®¡æ‘˜è¦:\n" + "\n".join(stats)
        
        # åŸå§‹æ•°æ®ï¼ˆé™åˆ¶è¡Œæ•°ï¼‰
        raw = data.head(10).to_string()
        
        return f"""
## å®éªŒæ•°æ®
{title}

{preview}

## åŸå§‹æ•°æ®ï¼ˆéƒ¨åˆ†ï¼‰
{raw}
"""
    
    def analyze_phenomenon(self, data: pd.DataFrame, title: str = "",
                           description: str = "") -> AnalysisResult:
        """åˆ†æå®éªŒç°è±¡"""
        if not self._available:
            return self._fallback_analysis(data, title)
        
        # æ ¼å¼åŒ–æ•°æ®
        data_str = self._format_data_for_ai(data, title)
        
        # æ„å»ºæç¤ºè¯
        prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¤§å­¦å®éªŒæŒ‡å¯¼è€å¸ˆã€‚è¯·åˆ†æä»¥ä¸‹å®éªŒæ•°æ®ï¼š

{data_str}

## ä»»åŠ¡
è¯·åˆ†æå®éªŒç°è±¡å¹¶æä¾›ï¼š
1. **å®éªŒç°è±¡æè¿°**ï¼šæ•°æ®å‘ˆç°ä»€ä¹ˆè§„å¾‹/è¶‹åŠ¿ï¼Ÿ
2. **å®éªŒç»“è®º**ï¼šæ ¹æ®æ•°æ®èƒ½å¾—å‡ºä»€ä¹ˆç»“è®ºï¼Ÿ
3. **æ•°æ®è¶‹åŠ¿**ï¼šæ˜¯çº¿æ€§/éçº¿æ€§ï¼Ÿå¢é•¿/ä¸‹é™ï¼Ÿ
4. **å¼‚å¸¸æ£€æµ‹**ï¼šæ˜¯å¦æœ‰å¼‚å¸¸æ•°æ®ç‚¹ï¼Ÿ
5. **æ”¹è¿›å»ºè®®**ï¼šå¦‚ä½•æ”¹è¿›å®éªŒï¼Ÿ

è¯·ç”¨ä¸­æ–‡å›å¤ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```
ç°è±¡: <æè¿°>
ç»“è®º: <ç»“è®º>
è¶‹åŠ¿: <è¶‹åŠ¿>
å¼‚å¸¸: <å¼‚å¸¸æƒ…å†µæˆ–"æ— ">
å»ºè®®: <æ”¹è¿›å»ºè®®>
ç½®ä¿¡åº¦: <0-1ä¹‹é—´çš„æ•°å­—>
```
"""
        try:
            messages = [{"role": "user", "content": prompt}]
            response = self.provider.chat(messages)
            
            # è§£æå“åº”
            result = self._parse_response(response)
            result.raw_response = response
            
            return result
            
        except Exception as e:
            print(f"âŒ AI åˆ†æå¤±è´¥: {e}")
            return self._fallback_analysis(data, title)
    
    def generate_conclusion(self, data: pd.DataFrame, experiment_type: str,
                            title: str = "") -> str:
        """ç”Ÿæˆå®éªŒç»“è®º"""
        if not self._available:
            return self._default_conclusion(data, experiment_type)
        
        data_str = self._format_data_for_ai(data, title)
        
        prompt = f"""
ä½ æ˜¯å¤§å­¦å®éªŒæŠ¥å‘Šå†™ä½œä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹å®éªŒæ•°æ®ç”Ÿæˆè§„èŒƒçš„å®éªŒç»“è®ºï¼š

{data_str}

å®éªŒç±»å‹: {experiment_type}

è¯·ç”Ÿæˆä¸€æ®µå®Œæ•´çš„å®éªŒç»“è®ºï¼ˆçº¦200å­—ï¼‰ï¼ŒåŒ…å«ï¼š
1. å®éªŒç›®çš„çš„è¾¾æˆæƒ…å†µ
2. æ•°æ®åˆ†æçš„ä¸»è¦å‘ç°
3. å®éªŒç»“æœçš„å¯é æ€§è¯„ä»·

è¯·ç”¨ä¸­æ–‡å›å¤ï¼Œç›´æ¥è¿”å›ç»“è®ºå†…å®¹ï¼Œæ— éœ€é¢å¤–è§£é‡Šã€‚
"""
        try:
            messages = [{"role": "user", "content": prompt}]
            return self.provider.chat(messages)
        except Exception as e:
            print(f"âŒ ç»“è®ºç”Ÿæˆå¤±è´¥: {e}")
            return self._default_conclusion(data, experiment_type)
    
    def fill_template_content(self, template_fields: Dict[str, str],
                             data: pd.DataFrame, title: str = "") -> Dict[str, str]:
        """å¡«å……æ¨¡æ¿å†…å®¹"""
        # åˆ†ææ•°æ®
        analysis = self.analyze_phenomenon(data, title)
        
        # å¡«å……å†…å®¹
        filled = {}
        for field, description in template_fields.items():
            if "ç»“è®º" in field:
                filled[field] = analysis.conclusion or "è¯·å¡«å†™ç»“è®º..."
            elif "ç°è±¡" in field:
                filled[field] = analysis.phenomenon or "è¯·åˆ†æç°è±¡..."
            elif "åˆ†æ" in field:
                filled[field] = analysis.phenomenon or "è¯·è¿›è¡Œåˆ†æ..."
            elif "å»ºè®®" in field:
                filled[field] = analysis.suggestion or "è¯·æå‡ºæ”¹è¿›å»ºè®®..."
            elif "è¯¯å·®" in field:
                filled[field] = self._generate_error_analysis(data)
            else:
                filled[field] = description
        
        return filled
    
    def _generate_error_analysis(self, data: pd.DataFrame) -> str:
        """ç”Ÿæˆè¯¯å·®åˆ†æ"""
        numeric_cols = data.select_dtypes(include=['number']).columns
        if not numeric_cols.empty:
            col = numeric_cols[0]
            col_data = data[col]
            cv = col_data.std() / col_data.mean() * 100 if col_data.mean() != 0 else 0
            return f"ç›¸å¯¹è¯¯å·®çº¦ä¸º {cv:.2f}%ï¼Œåœ¨å¯æ¥å—èŒƒå›´å†…ã€‚"
        return "è¯·æ‰‹åŠ¨åˆ†æè¯¯å·®æ¥æº..."
    
    def _parse_response(self, response: str) -> AnalysisResult:
        """è§£æ AI å“åº”"""
        result = AnalysisResult()
        
        lines = response.strip().split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith("ç°è±¡:"):
                result.phenomenon = line[3:].strip()
            elif line.startswith("ç»“è®º:"):
                result.conclusion = line[3:].strip()
            elif line.startswith("è¶‹åŠ¿:"):
                result.trend = line[3:].strip()
            elif line.startswith("å¼‚å¸¸:"):
                result.anomaly = line[3:].strip()
            elif line.startswith("å»ºè®®:"):
                result.suggestion = line[3:].strip()
            elif line.startswith("ç½®ä¿¡åº¦:"):
                try:
                    result.confidence = float(line[4:].strip())
                except:
                    result.confidence = 0.5
        
        return result
    
    def _fallback_analysis(self, data: pd.DataFrame, title: str) -> AnalysisResult:
        """é™çº§åˆ†æï¼ˆæ—  API æ—¶ï¼‰"""
        result = AnalysisResult()
        
        if data is not None and not data.empty:
            numeric_cols = data.select_dtypes(include=['number']).columns
            if len(numeric_cols) >= 2:
                x, y = numeric_cols[0], numeric_cols[1]
                x_data, y_data = data[x], data[y]
                
                # ç®€å•è¶‹åŠ¿åˆ†æ
                if x_data.diff().mean() > 0 and y_data.diff().mean() > 0:
                    trend = "æ­£ç›¸å…³è¶‹åŠ¿"
                elif x_data.diff().mean() > 0 and y_data.diff().mean() < 0:
                    trend = "è´Ÿç›¸å…³è¶‹åŠ¿"
                else:
                    trend = "æ— æ˜æ˜¾è¶‹åŠ¿"
                
                # å¼‚å¸¸æ£€æµ‹
                from numpy import abs as np_abs
                outliers = np_abs(y_data - y_data.mean()) > 2 * y_data.std()
                outlier_count = outliers.sum()
                
                result.phenomenon = f"å®éªŒæ•°æ®è¦†ç›– {x} èŒƒå›´ä¸º {x_data.min():.2f} ~ {x_data.max():.2f}"
                result.trend = trend
                result.anomaly = f"æ£€æµ‹åˆ° {outlier_count} ä¸ªæ½œåœ¨å¼‚å¸¸ç‚¹" if outlier_count > 0 else "æœªæ£€æµ‹åˆ°æ˜æ˜¾å¼‚å¸¸"
                result.conclusion = f"å®éªŒç»“æœä¸{x}å’Œ{y}çš„å…³ç³»ç›¸ç¬¦"
                result.suggestion = "å»ºè®®å¢åŠ æ•°æ®ç‚¹ä»¥æé«˜æ‹Ÿåˆç²¾åº¦"
                result.confidence = 0.7
        
        result.raw_response = "ï¼ˆæœ¬åœ°åˆ†ææ¨¡å¼ï¼‰"
        return result
    
    def _default_conclusion(self, data: pd.DataFrame, experiment_type: str) -> str:
        """é»˜è®¤ç»“è®º"""
        return f"""
æœ¬æ¬¡{experiment_type}å®éªŒå·²å®Œæˆï¼Œæ•°æ®é‡‡é›†å’Œåˆ†æå·¥ä½œå·²å°±ç»ªã€‚
æ ¹æ®å®éªŒæ•°æ®ï¼Œå¯å¾—å‡ºåˆæ­¥ç»“è®ºï¼š
1. å®éªŒæ•°æ®ç¬¦åˆç†è®ºé¢„æœŸ
2. ç›¸å¯¹è¯¯å·®åœ¨å¯æ¥å—èŒƒå›´å†…
3. å®éªŒæ–¹æ³•åˆç†ï¼Œç»“æœå¯é 

å»ºè®®åç»­å¯è¿›ä¸€æ­¥ä¼˜åŒ–å®éªŒæ¡ä»¶ï¼Œæé«˜æµ‹é‡ç²¾åº¦ã€‚
"""


# ä¾¿æ·å‡½æ•°
def quick_analyze(data_path: str, title: str = "",
                  api_key: str = "", provider: str = "openai") -> AnalysisResult:
    """å¿«é€Ÿåˆ†æå®éªŒæ•°æ®"""
    config = AIConfig(
        provider=provider,
        api_key=api_key or OPENAI_API_KEY
    )
    
    analyzer = AILabAnalyzer(config)
    
    # åŠ è½½æ•°æ®
    ext = Path(data_path).suffix.lower()
    if ext == '.csv':
        data = pd.read_csv(data_path)
    elif ext == '.xlsx':
        data = pd.read_excel(data_path)
    else:
        raise ValueError(f"ä¸æ”¯æŒæ ¼å¼: {ext}")
    
    return analyzer.analyze_phenomenon(data, title)


if __name__ == "__main__":
    # æµ‹è¯•
    import sys
    data_path = sys.argv[1] if len(sys.argv) > 1 else "data/examples/æ¬§å§†å®šå¾‹æ•°æ®.csv"
    
    # æœ¬åœ°æ¨¡å¼æµ‹è¯•ï¼ˆæ—  API Keyï¼‰
    analyzer = AILabAnalyzer(AIConfig(provider="local", base_url="http://localhost:11434/v1"))
    
    if not analyzer._available:
        print("âš ï¸ AI ä¸å¯ç”¨ï¼Œä½¿ç”¨æœ¬åœ°åˆ†æ...")
        analyzer = AILabAnalyzer()
    
    data = pd.read_csv(data_path)
    result = analyzer.analyze_phenomenon(data, "æ¬§å§†å®šå¾‹éªŒè¯å®éªŒ")
    
    print("\nğŸ“Š åˆ†æç»“æœ:")
    print(f"ç°è±¡: {result.phenomenon}")
    print(f"ç»“è®º: {result.conclusion}")
    print(f"è¶‹åŠ¿: {result.trend}")
    print(f"å¼‚å¸¸: {result.anomaly}")
    print(f"å»ºè®®: {result.suggestion}")
    print(f"ç½®ä¿¡åº¦: {result.confidence:.2f}")
